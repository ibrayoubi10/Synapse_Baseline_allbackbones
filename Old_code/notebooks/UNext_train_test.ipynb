{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Import"
      ],
      "metadata": {
        "id": "l6cDsgPYyucS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azvoS2kMxlz4",
        "outputId": "9737b640-bc0c-48b9-f8e7-2a584c49edad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'synapse' dataset.\n",
            "Path to dataset files: /kaggle/input/synapse\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"dogcdt/synapse\")\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Losses and Metrics"
      ],
      "metadata": {
        "id": "zMiDSBoQyw4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install medpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np_2BIhLx270",
        "outputId": "de1e5bad-8c30-4a15-86f4-83ac9b1f10b9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: medpy in /usr/local/lib/python3.12/dist-packages (0.5.2)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.12/dist-packages (from medpy) (1.16.3)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.12/dist-packages (from medpy) (2.0.2)\n",
            "Requirement already satisfied: SimpleITK>=2.1 in /usr/local/lib/python3.12/dist-packages (from medpy) (2.5.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from medpy import metric\n",
        "from scipy.ndimage import zoom\n",
        "import torch.nn as nn\n",
        "import SimpleITK as sitk\n",
        "\n",
        "## metrics\n",
        "def calculate_metric_percase(pred, gt):\n",
        "    pred[pred > 0] = 1\n",
        "    gt[gt > 0] = 1\n",
        "    if pred.sum() > 0 and gt.sum()>0:\n",
        "        dice = metric.binary.dc(pred, gt)\n",
        "        hd95 = metric.binary.hd95(pred, gt)\n",
        "        jac = metric.binary.jc(pred, gt)\n",
        "        return dice, jac, hd95\n",
        "    elif pred.sum() > 0 and gt.sum()==0:\n",
        "        return 1, 1, 0\n",
        "    else:\n",
        "        return 0, 0, 0\n",
        "\n",
        "def multilabel_metric(pred, gt, num_classes):\n",
        "    gt = gt.squeeze(0)\n",
        "    metric_list = []\n",
        "    for i in range(1, num_classes):\n",
        "        metric_list.append(calculate_metric_percase(pred == i, gt == i))\n",
        "    return metric_list  # list, lenth=num_classes-1, 每个元素含有(x,y,z)\n",
        "\n",
        "# losses\n",
        "class BinaryDiceLoss(nn.Module):\n",
        "    def init(self):\n",
        "        super(BinaryDiceLoss, self).init()\n",
        "\n",
        "    def forward(self, input, targets):\n",
        "        # 获取每个批次的大小 N\n",
        "        N = targets.size()[0]\n",
        "        # 平滑变量\n",
        "        smooth = 1\n",
        "        # 将宽高 reshape 到同一纬度\n",
        "        input_flat = input.view(N, -1)\n",
        "        targets_flat = targets.view(N, -1)\n",
        "        # 计算交集\n",
        "        intersection = input_flat * targets_flat\n",
        "        N_dice_eff = (2 * intersection.sum(1) + smooth) / (input_flat.sum(1) + targets_flat.sum(1) + smooth)\n",
        "        # 计算一个批次中平均每张图的损失\n",
        "        loss = 1 - N_dice_eff.sum() / N\n",
        "        return loss\n",
        "\n",
        "\n",
        "class MultiClassDiceLoss(nn.Module):\n",
        "    def init(self, weight=None, ignore_index=None, **kwargs):\n",
        "        super(MultiClassDiceLoss, self).init()\n",
        "        self.weight = weight\n",
        "        self.ignore_index = ignore_index\n",
        "        self.kwargs = kwargs\n",
        "    def forward(self, input, target):\n",
        "        \"\"\"\n",
        "        input tesor of shape = (N, C, H, W)\n",
        "        target tensor of shape = (N, H, W)\n",
        "        \"\"\"\n",
        "        # 先将 target 进行 one-hot 处理，转换为 (N, C, H, W)\n",
        "        # target[target==255.] = 0.\n",
        "        nclass = input.shape[1]\n",
        "        target = F.one_hot(target.long(), nclass)\n",
        "        target = target.reshape(input.shape[0],input.shape[1],input.shape[2],-1)\n",
        "\n",
        "        assert input.shape == target.shape, \"predict & target shape do not match\"\n",
        "        binaryDiceLoss = BinaryDiceLoss()\n",
        "        total_loss = 0\n",
        "        # 归一化输出\n",
        "        logits = F.softmax(input, dim=1)\n",
        "        C = target.shape[1]\n",
        "        # 遍历 channel，得到每个类别的二分类 DiceLoss\n",
        "        for i in range(C):\n",
        "            dice_loss = binaryDiceLoss(logits[:, i], target[:, i])\n",
        "            total_loss += dice_loss\n",
        "            # 每个类别的平均 dice_loss\n",
        "        return total_loss / C"
      ],
      "metadata": {
        "id": "Q9XSKWxMy17i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataSet implementation"
      ],
      "metadata": {
        "id": "Ntu9BTyky5Sp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import h5py\n",
        "import numpy as np\n",
        "import torch\n",
        "from scipy import ndimage\n",
        "from scipy.ndimage import zoom\n",
        "from torch.utils.data import Dataset\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Augmentations\n",
        "# -------------------------\n",
        "def random_rot_flip(image, label):\n",
        "    k = np.random.randint(0, 4)\n",
        "    image = np.rot90(image, k)\n",
        "    label = np.rot90(label, k)\n",
        "    axis = np.random.randint(0, 2)\n",
        "    image = np.flip(image, axis=axis).copy()\n",
        "    label = np.flip(label, axis=axis).copy()\n",
        "    return image, label\n",
        "\n",
        "\n",
        "def random_rotate(image, label):\n",
        "    angle = np.random.randint(-20, 20)\n",
        "    image = ndimage.rotate(image, angle, order=0, reshape=False)\n",
        "    label = ndimage.rotate(label, angle, order=0, reshape=False)\n",
        "    return image, label\n",
        "\n",
        "\n",
        "class RandomGenerator(object):\n",
        "    def __init__(self, output_size):\n",
        "        self.output_size = output_size  # (H, W)\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, label = sample[\"image\"], sample[\"label\"]\n",
        "\n",
        "        if random.random() > 0.5:\n",
        "            image, label = random_rot_flip(image, label)\n",
        "        elif random.random() > 0.5:\n",
        "            image, label = random_rotate(image, label)\n",
        "\n",
        "        x, y = image.shape\n",
        "        if (x != self.output_size[0]) or (y != self.output_size[1]):\n",
        "            image = zoom(image, (self.output_size[0] / x, self.output_size[1] / y), order=3)\n",
        "            label = zoom(label, (self.output_size[0] / x, self.output_size[1] / y), order=0)\n",
        "\n",
        "        image = torch.from_numpy(image.astype(np.float32)).unsqueeze(0)  # (1, H, W)\n",
        "        label = torch.from_numpy(label.astype(np.int64))                 # (H, W)\n",
        "        return {\"image\": image, \"label\": label}\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Dataset\n",
        "# -------------------------\n",
        "class Synapse_dataset(Dataset):\n",
        "    \"\"\"\n",
        "    Your structure:\n",
        "\n",
        "    /root/.cache/kagglehub/datasets/dogcdt/synapse/versions/1/Synapse/\n",
        "        train_npz/\n",
        "        test_vol_h5/   (or whatever your test folder is)\n",
        "    /root/lists_synapse/\n",
        "        train.txt\n",
        "        test_vol.txt\n",
        "\n",
        "    Train slices: <name>.npz with keys (image, label)\n",
        "    Test volumes: <name>.npy.h5 OR <name>.h5 OR <name>.n5 with datasets (image, label)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        versions_root=\"/kaggle/input/synapse\",\n",
        "        split=\"train\",\n",
        "        transform=None,\n",
        "        list_dir=\"/root/lists_synapse\",\n",
        "        data_subdir=\"Synapse\",          # <-- IMPORTANT: your extra folder level\n",
        "        train_folder=\"train_npz\",\n",
        "        test_folder=\"test_vol_h5\",\n",
        "        train_list=\"train.txt\",\n",
        "        test_list=\"test_vol.txt\",\n",
        "        verbose=True,\n",
        "    ):\n",
        "        self.transform = transform\n",
        "        self.split = split.lower().strip()\n",
        "\n",
        "        versions_root = Path(versions_root).expanduser().resolve()\n",
        "        self.data_root = (versions_root / data_subdir).resolve()  # <-- /.../versions/1/Synapse\n",
        "\n",
        "        list_dir = Path(list_dir).expanduser()\n",
        "        self.list_dir = list_dir.resolve() if list_dir.is_absolute() else (self.data_root / list_dir).resolve()\n",
        "\n",
        "        self.train_dir = (self.data_root / train_folder).resolve()\n",
        "        self.test_dir = (self.data_root / test_folder).resolve()\n",
        "\n",
        "        list_file = self.list_dir / (train_list if self.split == \"train\" else test_list)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"VERSIONS ROOT:\", versions_root)\n",
        "            print(\"DATA ROOT    :\", self.data_root)\n",
        "            print(\"LIST DIR     :\", self.list_dir)\n",
        "            print(\"LIST FILE    :\", list_file)\n",
        "            print(\"TRAIN DIR    :\", self.train_dir)\n",
        "            print(\"TEST  DIR    :\", self.test_dir)\n",
        "\n",
        "        if not list_file.is_file():\n",
        "            raise FileNotFoundError(f\"Missing list file: {list_file}\")\n",
        "\n",
        "        self.sample_list = list_file.read_text().splitlines()\n",
        "\n",
        "        if self.split == \"train\" and not self.train_dir.is_dir():\n",
        "            raise FileNotFoundError(f\"Missing folder: {self.train_dir}\")\n",
        "        if self.split != \"train\" and not self.test_dir.is_dir():\n",
        "            raise FileNotFoundError(f\"Missing folder: {self.test_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sample_list)\n",
        "\n",
        "    def _resolve_train_path(self, name: str) -> Path:\n",
        "        return self.train_dir / f\"{name}.npz\"\n",
        "\n",
        "    def _resolve_test_path(self, name: str) -> Path:\n",
        "        candidates = [\n",
        "            self.test_dir / f\"{name}.npy.h5\",\n",
        "            self.test_dir / f\"{name}.h5\",\n",
        "            self.test_dir / f\"{name}.n5\",\n",
        "        ]\n",
        "        for p in candidates:\n",
        "            if p.is_file():\n",
        "                return p\n",
        "        return candidates[0]\n",
        "\n",
        "    def show_paths(self, max_items=20):\n",
        "        n = min(len(self.sample_list), max_items)\n",
        "        print(f\"Showing {n}/{len(self.sample_list)} paths for split='{self.split}'\")\n",
        "        for i in range(n):\n",
        "            name = self.sample_list[i].strip()\n",
        "            p = self._resolve_train_path(name) if self.split == \"train\" else self._resolve_test_path(name)\n",
        "            print(p, \"| exists =\", p.exists())\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.sample_list[idx].strip()\n",
        "\n",
        "        if self.split == \"train\":\n",
        "            data_path = self._resolve_train_path(name)\n",
        "            if not data_path.is_file():\n",
        "                raise FileNotFoundError(f\"Missing train npz: {data_path}\")\n",
        "\n",
        "            data = np.load(str(data_path))\n",
        "            image, label = data[\"image\"], data[\"label\"]\n",
        "\n",
        "        else:\n",
        "            h5_path = self._resolve_test_path(name)\n",
        "            if not h5_path.is_file():\n",
        "                raise FileNotFoundError(\n",
        "                    \"Missing test volume. Tried: \"\n",
        "                    f\"{self.test_dir / (name + '.npy.h5')}, \"\n",
        "                    f\"{self.test_dir / (name + '.h5')}, \"\n",
        "                    f\"{self.test_dir / (name + '.n5')}\"\n",
        "                )\n",
        "\n",
        "            with h5py.File(str(h5_path), \"r\") as data:\n",
        "                image, label = data[\"image\"][:], data[\"label\"][:]\n",
        "\n",
        "        sample = {\"image\": image, \"label\": label}\n",
        "        if self.transform is not None:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        sample[\"case_name\"] = name\n",
        "        return sample\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Quick local test\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    versions_root = \"/kaggle/input/synapse\"\n",
        "    list_dir = \"/root/lists_synapse\"\n",
        "\n",
        "    ds_train = Synapse_dataset(versions_root=versions_root, split=\"train\", transform=None, list_dir=list_dir)\n",
        "    ds_train.show_paths(max_items=30)\n",
        "\n",
        "    ds_test = Synapse_dataset(versions_root=versions_root, split=\"test\", transform=None, list_dir=list_dir)\n",
        "    ds_test.show_paths(max_items=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htb6rzfoy4q6",
        "outputId": "b9e4f178-dd37-4f3f-c7cf-258b91412c4e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VERSIONS ROOT: /kaggle/input/synapse\n",
            "DATA ROOT    : /kaggle/input/synapse/Synapse\n",
            "LIST DIR     : /root/lists_synapse\n",
            "LIST FILE    : /root/lists_synapse/train.txt\n",
            "TRAIN DIR    : /kaggle/input/synapse/Synapse/train_npz\n",
            "TEST  DIR    : /kaggle/input/synapse/Synapse/test_vol_h5\n",
            "Showing 30/2211 paths for split='train'\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice000.npz | exists = True\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice001.npz | exists = True\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice002.npz | exists = True\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice003.npz | exists = True\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice004.npz | exists = True\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice005.npz | exists = True\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice006.npz | exists = True\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice007.npz | exists = True\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice008.npz | exists = True\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice009.npz | exists = True\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice010.npz | exists = True\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice011.npz | exists = True\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice012.npz | exists = True\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice013.npz | exists = True\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice014.npz | exists = True\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice015.npz | exists = True\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice016.npz | exists = True\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice017.npz | exists = True\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice018.npz | exists = True\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice019.npz | exists = True\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice020.npz | exists = True\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice021.npz | exists = True\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice022.npz | exists = True\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice023.npz | exists = True\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice024.npz | exists = True\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice025.npz | exists = True\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice026.npz | exists = True\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice027.npz | exists = True\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice028.npz | exists = True\n",
            "/kaggle/input/synapse/Synapse/train_npz/case0031_slice029.npz | exists = True\n",
            "VERSIONS ROOT: /kaggle/input/synapse\n",
            "DATA ROOT    : /kaggle/input/synapse/Synapse\n",
            "LIST DIR     : /root/lists_synapse\n",
            "LIST FILE    : /root/lists_synapse/test_vol.txt\n",
            "TRAIN DIR    : /kaggle/input/synapse/Synapse/train_npz\n",
            "TEST  DIR    : /kaggle/input/synapse/Synapse/test_vol_h5\n",
            "Showing 10/12 paths for split='test'\n",
            "/kaggle/input/synapse/Synapse/test_vol_h5/case0008.npy.h5 | exists = True\n",
            "/kaggle/input/synapse/Synapse/test_vol_h5/case0022.npy.h5 | exists = True\n",
            "/kaggle/input/synapse/Synapse/test_vol_h5/case0038.npy.h5 | exists = True\n",
            "/kaggle/input/synapse/Synapse/test_vol_h5/case0036.npy.h5 | exists = True\n",
            "/kaggle/input/synapse/Synapse/test_vol_h5/case0032.npy.h5 | exists = True\n",
            "/kaggle/input/synapse/Synapse/test_vol_h5/case0002.npy.h5 | exists = True\n",
            "/kaggle/input/synapse/Synapse/test_vol_h5/case0029.npy.h5 | exists = True\n",
            "/kaggle/input/synapse/Synapse/test_vol_h5/case0003.npy.h5 | exists = True\n",
            "/kaggle/input/synapse/Synapse/test_vol_h5/case0001.npy.h5 | exists = True\n",
            "/kaggle/input/synapse/Synapse/test_vol_h5/case0004.npy.h5 | exists = True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unext Architecture"
      ],
      "metadata": {
        "id": "KBVaURSf0H9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U openmim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHYkDSDMz3X7",
        "outputId": "890e7e97-a6f8-44bf-878e-62306c1444a2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openmim in /usr/local/lib/python3.12/dist-packages (0.3.9)\n",
            "Requirement already satisfied: Click in /usr/local/lib/python3.12/dist-packages (from openmim) (8.3.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from openmim) (0.4.6)\n",
            "Requirement already satisfied: model-index in /usr/local/lib/python3.12/dist-packages (from openmim) (0.1.11)\n",
            "Requirement already satisfied: opendatalab in /usr/local/lib/python3.12/dist-packages (from openmim) (0.0.10)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from openmim) (2.2.2)\n",
            "Requirement already satisfied: pip>=19.3 in /usr/local/lib/python3.12/dist-packages (from openmim) (24.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from openmim) (2.28.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from openmim) (13.4.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from openmim) (0.9.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from model-index->openmim) (6.0.3)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.12/dist-packages (from model-index->openmim) (3.10.2)\n",
            "Requirement already satisfied: ordered-set in /usr/local/lib/python3.12/dist-packages (from model-index->openmim) (4.1.0)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.12/dist-packages (from opendatalab->openmim) (3.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from opendatalab->openmim) (4.65.2)\n",
            "Requirement already satisfied: openxlab in /usr/local/lib/python3.12/dist-packages (from opendatalab->openmim) (0.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->openmim) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->openmim) (3.11)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->openmim) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->openmim) (2026.1.4)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas->openmim) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->openmim) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->openmim) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->openmim) (2025.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->openmim) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->openmim) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->openmim) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->openmim) (1.17.0)\n",
            "Requirement already satisfied: filelock~=3.14.0 in /usr/local/lib/python3.12/dist-packages (from openxlab->opendatalab->openmim) (3.14.0)\n",
            "Requirement already satisfied: oss2~=2.17.0 in /usr/local/lib/python3.12/dist-packages (from openxlab->opendatalab->openmim) (2.17.0)\n",
            "Requirement already satisfied: packaging~=24.0 in /usr/local/lib/python3.12/dist-packages (from openxlab->opendatalab->openmim) (24.2)\n",
            "Requirement already satisfied: setuptools~=60.2.0 in /usr/local/lib/python3.12/dist-packages (from openxlab->opendatalab->openmim) (60.2.0)\n",
            "Requirement already satisfied: crcmod>=1.7 in /usr/local/lib/python3.12/dist-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim) (1.7)\n",
            "Requirement already satisfied: aliyun-python-sdk-kms>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim) (2.16.5)\n",
            "Requirement already satisfied: aliyun-python-sdk-core>=2.13.12 in /usr/local/lib/python3.12/dist-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim) (2.16.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (0.10.0)\n",
            "Requirement already satisfied: cryptography>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=3.0.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=3.0.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U mmcv-lite"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCcSK7HP4fLd",
        "outputId": "57bef290-f45a-418f-d14e-854a7dc3536f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mmcv-lite\n",
            "  Downloading mmcv_lite-2.2.0-py2.py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting addict (from mmcv-lite)\n",
            "  Using cached addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting mmengine>=0.3.0 (from mmcv-lite)\n",
            "  Using cached mmengine-0.10.7-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from mmcv-lite) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from mmcv-lite) (24.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from mmcv-lite) (11.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from mmcv-lite) (6.0.3)\n",
            "Collecting yapf (from mmcv-lite)\n",
            "  Using cached yapf-0.43.0-py3-none-any.whl.metadata (46 kB)\n",
            "Requirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.12/dist-packages (from mmcv-lite) (4.13.0.92)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mmengine>=0.3.0->mmcv-lite) (3.10.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from mmengine>=0.3.0->mmcv-lite) (13.4.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from mmengine>=0.3.0->mmcv-lite) (3.3.0)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.12/dist-packages (from yapf->mmcv-lite) (4.9.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv-lite) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv-lite) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv-lite) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv-lite) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv-lite) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv-lite) (2.9.0.post0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->mmengine>=0.3.0->mmcv-lite) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->mmengine>=0.3.0->mmcv-lite) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->mmengine>=0.3.0->mmcv-lite) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mmengine>=0.3.0->mmcv-lite) (1.17.0)\n",
            "Downloading mmcv_lite-2.2.0-py2.py3-none-any.whl (732 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m732.3/732.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached mmengine-0.10.7-py3-none-any.whl (452 kB)\n",
            "Using cached addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Using cached yapf-0.43.0-py3-none-any.whl (256 kB)\n",
            "Installing collected packages: addict, yapf, mmengine, mmcv-lite\n",
            "Successfully installed addict-2.4.0 mmcv-lite-2.2.0 mmengine-0.10.7 yapf-0.43.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 version premier version avec mmcv et une deuxieme sans"
      ],
      "metadata": {
        "id": "NaP0m-vH60im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#from utils import *\n",
        "__all__ = ['UNext']\n",
        "\n",
        "import timm\n",
        "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
        "import types\n",
        "import math\n",
        "from abc import ABCMeta, abstractmethod\n",
        "from mmcv.cnn import ConvModule\n",
        "import pdb\n",
        "\n",
        "\n",
        "\n",
        "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, bias=False)\n",
        "\n",
        "\n",
        "# def shift(dim):\n",
        "#     x_shift = [ torch.roll(x_c, shift, dim) for x_c, shift in zip(xs, range(-self.pad, self.pad +1))]\n",
        "#     x_cat = torch.cat(x_shift, 1)\n",
        "#     x_cat = torch.narrow(x_cat, 2, self.pad, H)\n",
        "#     x_cat = torch.narrow(x_cat, 3, self.pad, W)\n",
        "#     return x_cat\n",
        "\n",
        "class shiftmlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0., shift_size=5):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.dim = in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.dwconv = DWConv(hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "        self.shift_size = shift_size\n",
        "        self.pad = shift_size // 2\n",
        "\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.Conv2d):\n",
        "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "            fan_out //= m.groups\n",
        "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    #     def shift(x, dim):\n",
        "    #         x = F.pad(x, \"constant\", 0)\n",
        "    #         x = torch.chunk(x, shift_size, 1)\n",
        "    #         x = [ torch.roll(x_c, shift, dim) for x_s, shift in zip(x, range(-pad, pad+1))]\n",
        "    #         x = torch.cat(x, 1)\n",
        "    #         return x[:, :, pad:-pad, pad:-pad]\n",
        "\n",
        "    def forward(self, x, H, W):\n",
        "        # pdb.set_trace()\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        xn = x.transpose(1, 2).view(B, C, H, W).contiguous()\n",
        "        xn = F.pad(xn, (self.pad, self.pad, self.pad, self.pad) , \"constant\", 0)\n",
        "        xs = torch.chunk(xn, self.shift_size, 1)\n",
        "        x_shift = [torch.roll(x_c, shift, 2) for x_c, shift in zip(xs, range(-self.pad, self.pad +1))]\n",
        "        x_cat = torch.cat(x_shift, 1)\n",
        "        x_cat = torch.narrow(x_cat, 2, self.pad, H)\n",
        "        x_s = torch.narrow(x_cat, 3, self.pad, W)\n",
        "\n",
        "\n",
        "        x_s = x_s.reshape(B ,C , H *W).contiguous()\n",
        "        x_shift_r = x_s.transpose(1 ,2)\n",
        "\n",
        "\n",
        "        x = self.fc1(x_shift_r)\n",
        "\n",
        "        x = self.dwconv(x, H, W)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        xn = x.transpose(1, 2).view(B, C, H, W).contiguous()\n",
        "        xn = F.pad(xn, (self.pad, self.pad, self.pad, self.pad) , \"constant\", 0)\n",
        "        xs = torch.chunk(xn, self.shift_size, 1)\n",
        "        x_shift = [torch.roll(x_c, shift, 3) for x_c, shift in zip(xs, range(-self.pad, self.pad +1))]\n",
        "        x_cat = torch.cat(x_shift, 1)\n",
        "        x_cat = torch.narrow(x_cat, 2, self.pad, H)\n",
        "        x_s = torch.narrow(x_cat, 3, self.pad, W)\n",
        "        x_s = x_s.reshape(B ,C , H *W).contiguous()\n",
        "        x_shift_c = x_s.transpose(1 ,2)\n",
        "\n",
        "        x = self.fc2(x_shift_c)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class shiftedBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = shiftmlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.Conv2d):\n",
        "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "            fan_out //= m.groups\n",
        "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x, H, W):\n",
        "\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x), H, W))\n",
        "        return x\n",
        "\n",
        "\n",
        "class DWConv(nn.Module):\n",
        "    def __init__(self, dim=768):\n",
        "        super(DWConv, self).__init__()\n",
        "        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n",
        "\n",
        "    def forward(self, x, H, W):\n",
        "        B, N, C = x.shape\n",
        "        x = x.transpose(1, 2).view(B, C, H, W)\n",
        "        x = self.dwconv(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "\n",
        "        return x\n",
        "\n",
        "class OverlapPatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        img_size = to_2tuple(img_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n",
        "        self.num_patches = self.H * self.W\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,\n",
        "                              padding=(patch_size[0] // 2, patch_size[1] // 2))\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.Conv2d):\n",
        "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "            fan_out //= m.groups\n",
        "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        _, _, H, W = x.shape\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return x, H, W\n",
        "\n",
        "\n",
        "class UNext(nn.Module):\n",
        "\n",
        "    ## Conv 3 + MLP 2 + shifted MLP\n",
        "\n",
        "    def __init__(self,  num_classes, input_channels=3, deep_supervision=False ,img_size=224, patch_size=16, in_chans=3,  embed_dims=[ 128, 160, 256],\n",
        "                 num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=False, qk_scale=None, drop_rate=0.,\n",
        "                 attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm,\n",
        "                 depths=[1, 1, 1], sr_ratios=[8, 4, 2, 1], **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder1 = nn.Conv2d(3, 16, 3, stride=1, padding=1)\n",
        "        self.encoder2 = nn.Conv2d(16, 32, 3, stride=1, padding=1)\n",
        "        self.encoder3 = nn.Conv2d(32, 128, 3, stride=1, padding=1)\n",
        "\n",
        "        self.ebn1 = nn.BatchNorm2d(16)\n",
        "        self.ebn2 = nn.BatchNorm2d(32)\n",
        "        self.ebn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.norm3 = norm_layer(embed_dims[1])\n",
        "        self.norm4 = norm_layer(embed_dims[2])\n",
        "\n",
        "        self.dnorm3 = norm_layer(160)\n",
        "        self.dnorm4 = norm_layer(128)\n",
        "\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
        "\n",
        "        self.block1 = nn.ModuleList([shiftedBlock(\n",
        "            dim=embed_dims[1], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], norm_layer=norm_layer,\n",
        "            sr_ratio=sr_ratios[0])])\n",
        "\n",
        "        self.block2 = nn.ModuleList([shiftedBlock(\n",
        "            dim=embed_dims[2], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[1], norm_layer=norm_layer,\n",
        "            sr_ratio=sr_ratios[0])])\n",
        "\n",
        "        self.dblock1 = nn.ModuleList([shiftedBlock(\n",
        "            dim=embed_dims[1], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], norm_layer=norm_layer,\n",
        "            sr_ratio=sr_ratios[0])])\n",
        "\n",
        "        self.dblock2 = nn.ModuleList([shiftedBlock(\n",
        "            dim=embed_dims[0], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[1], norm_layer=norm_layer,\n",
        "            sr_ratio=sr_ratios[0])])\n",
        "\n",
        "        self.patch_embed3 = OverlapPatchEmbed(img_size=img_size // 4, patch_size=3, stride=2, in_chans=embed_dims[0],\n",
        "                                              embed_dim=embed_dims[1])\n",
        "        self.patch_embed4 = OverlapPatchEmbed(img_size=img_size // 8, patch_size=3, stride=2, in_chans=embed_dims[1],\n",
        "                                              embed_dim=embed_dims[2])\n",
        "\n",
        "        self.decoder1 = nn.Conv2d(256, 160, 3, stride=1 ,padding=1)\n",
        "        self.decoder2 =   nn.Conv2d(160, 128, 3, stride=1, padding=1)\n",
        "        self.decoder3 =   nn.Conv2d(128, 32, 3, stride=1, padding=1)\n",
        "        self.decoder4 =   nn.Conv2d(32, 16, 3, stride=1, padding=1)\n",
        "        self.decoder5 =   nn.Conv2d(16, 16, 3, stride=1, padding=1)\n",
        "\n",
        "        self.dbn1 = nn.BatchNorm2d(160)\n",
        "        self.dbn2 = nn.BatchNorm2d(128)\n",
        "        self.dbn3 = nn.BatchNorm2d(32)\n",
        "        self.dbn4 = nn.BatchNorm2d(16)\n",
        "\n",
        "        self.final = nn.Conv2d(16, num_classes, kernel_size=1)\n",
        "\n",
        "        self.soft = nn.Softmax(dim =1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B = x.shape[0]\n",
        "        ### Encoder\n",
        "        ### Conv Stage\n",
        "\n",
        "        ### Stage 1\n",
        "        out = F.relu(F.max_pool2d(self.ebn1(self.encoder1(x)) ,2 ,2))\n",
        "        t1 = out\n",
        "        ### Stage 2\n",
        "        out = F.relu(F.max_pool2d(self.ebn2(self.encoder2(out)) ,2 ,2))\n",
        "        t2 = out\n",
        "        ### Stage 3\n",
        "        out = F.relu(F.max_pool2d(self.ebn3(self.encoder3(out)) ,2 ,2))\n",
        "        t3 = out\n",
        "\n",
        "        ### Tokenized MLP Stage\n",
        "        ### Stage 4\n",
        "\n",
        "        out ,H ,W = self.patch_embed3(out)\n",
        "        for i, blk in enumerate(self.block1):\n",
        "            out = blk(out, H, W)\n",
        "        out = self.norm3(out)\n",
        "        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
        "        t4 = out\n",
        "\n",
        "        ### Bottleneck\n",
        "\n",
        "        out ,H, W = self.patch_embed4(out)\n",
        "        for i, blk in enumerate(self.block2):\n",
        "            out = blk(out, H, W)\n",
        "        out = self.norm4(out)\n",
        "        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        ### Stage 4\n",
        "\n",
        "        out = F.relu(F.interpolate(self.dbn1(self.decoder1(out)), scale_factor=(2, 2), mode='bilinear'))\n",
        "\n",
        "        out = torch.add(out, t4)\n",
        "        _, _, H, W = out.shape\n",
        "        out = out.flatten(2).transpose(1, 2)\n",
        "        for i, blk in enumerate(self.dblock1):\n",
        "            out = blk(out, H, W)\n",
        "\n",
        "        ### Stage 3\n",
        "\n",
        "        out = self.dnorm3(out)\n",
        "        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
        "        out = F.relu(F.interpolate(self.dbn2(self.decoder2(out)), scale_factor=(2, 2), mode='bilinear'))\n",
        "        out = torch.add(out, t3)\n",
        "        _, _, H, W = out.shape\n",
        "        out = out.flatten(2).transpose(1, 2)\n",
        "\n",
        "        for i, blk in enumerate(self.dblock2):\n",
        "            out = blk(out, H, W)\n",
        "\n",
        "        out = self.dnorm4(out)\n",
        "        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        out = F.relu(F.interpolate(self.dbn3(self.decoder3(out)), scale_factor=(2, 2), mode='bilinear'))\n",
        "        out = torch.add(out, t2)\n",
        "        out = F.relu(F.interpolate(self.dbn4(self.decoder4(out)), scale_factor=(2, 2), mode='bilinear'))\n",
        "        out = torch.add(out, t1)\n",
        "        out = F.relu(F.interpolate(self.decoder5(out), scale_factor=(2, 2), mode='bilinear'))\n",
        "\n",
        "        # out = self.final(out)\n",
        "        # out = F.sigmoid(self.final(out).squeeze(1))\n",
        "        out = self.final(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class UNext_S(nn.Module):\n",
        "\n",
        "    ## Conv 3 + MLP 2 + shifted MLP w less parameters\n",
        "\n",
        "    def __init__(self, num_classes, input_channels=3, deep_supervision=False, img_size=224, patch_size=16, in_chans=3,\n",
        "                 embed_dims=[32, 64, 128, 512],\n",
        "                 num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=False, qk_scale=None, drop_rate=0.,\n",
        "                 attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm,\n",
        "                 depths=[1, 1, 1], sr_ratios=[8, 4, 2, 1], **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder1 = nn.Conv2d(3, 8, 3, stride=1, padding=1)\n",
        "        self.encoder2 = nn.Conv2d(8, 16, 3, stride=1, padding=1)\n",
        "        self.encoder3 = nn.Conv2d(16, 32, 3, stride=1, padding=1)\n",
        "\n",
        "        self.ebn1 = nn.BatchNorm2d(8)\n",
        "        self.ebn2 = nn.BatchNorm2d(16)\n",
        "        self.ebn3 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.norm3 = norm_layer(embed_dims[1])\n",
        "        self.norm4 = norm_layer(embed_dims[2])\n",
        "\n",
        "        self.dnorm3 = norm_layer(64)\n",
        "        self.dnorm4 = norm_layer(32)\n",
        "\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
        "\n",
        "        self.block1 = nn.ModuleList([shiftedBlock(\n",
        "            dim=embed_dims[1], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], norm_layer=norm_layer,\n",
        "            sr_ratio=sr_ratios[0])])\n",
        "\n",
        "        self.block2 = nn.ModuleList([shiftedBlock(\n",
        "            dim=embed_dims[2], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[1], norm_layer=norm_layer,\n",
        "            sr_ratio=sr_ratios[0])])\n",
        "\n",
        "        self.dblock1 = nn.ModuleList([shiftedBlock(\n",
        "            dim=embed_dims[1], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], norm_layer=norm_layer,\n",
        "            sr_ratio=sr_ratios[0])])\n",
        "\n",
        "        self.dblock2 = nn.ModuleList([shiftedBlock(\n",
        "            dim=embed_dims[0], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[1], norm_layer=norm_layer,\n",
        "            sr_ratio=sr_ratios[0])])\n",
        "\n",
        "        self.patch_embed3 = OverlapPatchEmbed(img_size=img_size // 4, patch_size=3, stride=2, in_chans=embed_dims[0],\n",
        "                                              embed_dim=embed_dims[1])\n",
        "        self.patch_embed4 = OverlapPatchEmbed(img_size=img_size // 8, patch_size=3, stride=2, in_chans=embed_dims[1],\n",
        "                                              embed_dim=embed_dims[2])\n",
        "\n",
        "        self.decoder1 = nn.Conv2d(128, 64, 3, stride=1, padding=1)\n",
        "        self.decoder2 = nn.Conv2d(64, 32, 3, stride=1, padding=1)\n",
        "        self.decoder3 = nn.Conv2d(32, 16, 3, stride=1, padding=1)\n",
        "        self.decoder4 = nn.Conv2d(16, 8, 3, stride=1, padding=1)\n",
        "        self.decoder5 = nn.Conv2d(8, 8, 3, stride=1, padding=1)\n",
        "\n",
        "        self.dbn1 = nn.BatchNorm2d(64)\n",
        "        self.dbn2 = nn.BatchNorm2d(32)\n",
        "        self.dbn3 = nn.BatchNorm2d(16)\n",
        "        self.dbn4 = nn.BatchNorm2d(8)\n",
        "\n",
        "        self.final = nn.Conv2d(8, num_classes, kernel_size=1)\n",
        "\n",
        "        self.soft = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B = x.shape[0]\n",
        "        ### Encoder\n",
        "        ### Conv Stage\n",
        "\n",
        "        ### Stage 1\n",
        "        out = F.relu(F.max_pool2d(self.ebn1(self.encoder1(x)), 2, 2))\n",
        "        t1 = out\n",
        "        ### Stage 2\n",
        "        out = F.relu(F.max_pool2d(self.ebn2(self.encoder2(out)), 2, 2))\n",
        "        t2 = out\n",
        "        ### Stage 3\n",
        "        out = F.relu(F.max_pool2d(self.ebn3(self.encoder3(out)), 2, 2))\n",
        "        t3 = out\n",
        "\n",
        "        ### Tokenized MLP Stage\n",
        "        ### Stage 4\n",
        "\n",
        "        out, H, W = self.patch_embed3(out)\n",
        "        for i, blk in enumerate(self.block1):\n",
        "            out = blk(out, H, W)\n",
        "        out = self.norm3(out)\n",
        "        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
        "        t4 = out\n",
        "\n",
        "        ### Bottleneck\n",
        "\n",
        "        out, H, W = self.patch_embed4(out)\n",
        "        for i, blk in enumerate(self.block2):\n",
        "            out = blk(out, H, W)\n",
        "        out = self.norm4(out)\n",
        "        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        ### Stage 4\n",
        "\n",
        "        out = F.relu(F.interpolate(self.dbn1(self.decoder1(out)), scale_factor=(2, 2), mode='bilinear'))\n",
        "\n",
        "        out = torch.add(out, t4)\n",
        "        _, _, H, W = out.shape\n",
        "        out = out.flatten(2).transpose(1, 2)\n",
        "        for i, blk in enumerate(self.dblock1):\n",
        "            out = blk(out, H, W)\n",
        "\n",
        "        ### Stage 3\n",
        "\n",
        "        out = self.dnorm3(out)\n",
        "        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
        "        out = F.relu(F.interpolate(self.dbn2(self.decoder2(out)), scale_factor=(2, 2), mode='bilinear'))\n",
        "        out = torch.add(out, t3)\n",
        "        _, _, H, W = out.shape\n",
        "        out = out.flatten(2).transpose(1, 2)\n",
        "\n",
        "        for i, blk in enumerate(self.dblock2):\n",
        "            out = blk(out, H, W)\n",
        "\n",
        "        out = self.dnorm4(out)\n",
        "        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        out = F.relu(F.interpolate(self.dbn3(self.decoder3(out)), scale_factor=(2, 2), mode='bilinear'))\n",
        "        out = torch.add(out, t2)\n",
        "        out = F.relu(F.interpolate(self.dbn4(self.decoder4(out)), scale_factor=(2, 2), mode='bilinear'))\n",
        "        out = torch.add(out, t1)\n",
        "        out = F.relu(F.interpolate(self.decoder5(out), scale_factor=(2, 2), mode='bilinear'))\n",
        "\n",
        "        return self.final(out)\n",
        "\n",
        "# EOF\n",
        "model = UNext(num_classes=1, input_channels=3, deep_supervision=True).cuda()\n",
        "inp = torch.rand((2, 3, 224, 224)).cuda()\n",
        "out = model(inp)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFS558NL0X3S",
        "outputId": "fb0402ac-0656-4e13-ac63-e866404c6411"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:49: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 1, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "__all__ = ['UNext', 'UNext_S']\n",
        "\n",
        "import timm\n",
        "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
        "import math\n",
        "\n",
        "\n",
        "class DWConv(nn.Module):\n",
        "    def __init__(self, dim=768):\n",
        "        super().__init__()\n",
        "        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n",
        "\n",
        "    def forward(self, x, H, W):\n",
        "        B, N, C = x.shape\n",
        "        x = x.transpose(1, 2).view(B, C, H, W)\n",
        "        x = self.dwconv(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class shiftmlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None,\n",
        "                 act_layer=nn.GELU, drop=0., shift_size=5):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "\n",
        "        self.dim = in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.dwconv = DWConv(hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "        self.shift_size = shift_size\n",
        "        self.pad = shift_size // 2\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.Conv2d):\n",
        "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "            fan_out //= m.groups\n",
        "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x, H, W):\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        xn = x.transpose(1, 2).view(B, C, H, W).contiguous()\n",
        "        xn = F.pad(xn, (self.pad, self.pad, self.pad, self.pad), \"constant\", 0)\n",
        "        xs = torch.chunk(xn, self.shift_size, 1)\n",
        "        x_shift = [torch.roll(x_c, shift, 2) for x_c, shift in zip(xs, range(-self.pad, self.pad + 1))]\n",
        "        x_cat = torch.cat(x_shift, 1)\n",
        "        x_cat = torch.narrow(x_cat, 2, self.pad, H)\n",
        "        x_s = torch.narrow(x_cat, 3, self.pad, W)\n",
        "        x_s = x_s.reshape(B, C, H * W).contiguous()\n",
        "        x_shift_r = x_s.transpose(1, 2)\n",
        "\n",
        "        x = self.fc1(x_shift_r)\n",
        "        x = self.dwconv(x, H, W)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        xn = x.transpose(1, 2).view(B, C, H, W).contiguous()\n",
        "        xn = F.pad(xn, (self.pad, self.pad, self.pad, self.pad), \"constant\", 0)\n",
        "        xs = torch.chunk(xn, self.shift_size, 1)\n",
        "        x_shift = [torch.roll(x_c, shift, 3) for x_c, shift in zip(xs, range(-self.pad, self.pad + 1))]\n",
        "        x_cat = torch.cat(x_shift, 1)\n",
        "        x_cat = torch.narrow(x_cat, 2, self.pad, H)\n",
        "        x_s = torch.narrow(x_cat, 3, self.pad, W)\n",
        "        x_s = x_s.reshape(B, C, H * W).contiguous()\n",
        "        x_shift_c = x_s.transpose(1, 2)\n",
        "\n",
        "        x = self.fc2(x_shift_c)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class shiftedBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False,\n",
        "                 qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
        "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1):\n",
        "        super().__init__()\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = shiftmlp(in_features=dim, hidden_features=mlp_hidden_dim,\n",
        "                            act_layer=act_layer, drop=drop)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.Conv2d):\n",
        "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "            fan_out //= m.groups\n",
        "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x, H, W):\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x), H, W))\n",
        "        return x\n",
        "\n",
        "\n",
        "class OverlapPatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        img_size = to_2tuple(img_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n",
        "        self.num_patches = self.H * self.W\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size,\n",
        "                              stride=stride, padding=(patch_size[0] // 2, patch_size[1] // 2))\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.Conv2d):\n",
        "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "            fan_out //= m.groups\n",
        "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        _, _, H, W = x.shape\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = self.norm(x)\n",
        "        return x, H, W\n",
        "\n",
        "\n",
        "class UNext(nn.Module):\n",
        "    def __init__(self, num_classes, input_channels=3, deep_supervision=False,\n",
        "                 img_size=224, patch_size=16, in_chans=3,\n",
        "                 embed_dims=[128, 160, 256],\n",
        "                 num_heads=[1, 2, 4, 8],\n",
        "                 mlp_ratios=[4, 4, 4, 4],\n",
        "                 qkv_bias=False, qk_scale=None, drop_rate=0.,\n",
        "                 attn_drop_rate=0., drop_path_rate=0.,\n",
        "                 norm_layer=nn.LayerNorm, depths=[1, 1, 1],\n",
        "                 sr_ratios=[8, 4, 2, 1], **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder1 = nn.Conv2d(3, 16, 3, stride=1, padding=1)\n",
        "        self.encoder2 = nn.Conv2d(16, 32, 3, stride=1, padding=1)\n",
        "        self.encoder3 = nn.Conv2d(32, 128, 3, stride=1, padding=1)\n",
        "\n",
        "        self.ebn1 = nn.BatchNorm2d(16)\n",
        "        self.ebn2 = nn.BatchNorm2d(32)\n",
        "        self.ebn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.norm3 = norm_layer(embed_dims[1])\n",
        "        self.norm4 = norm_layer(embed_dims[2])\n",
        "\n",
        "        self.dnorm3 = norm_layer(160)\n",
        "        self.dnorm4 = norm_layer(128)\n",
        "\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
        "\n",
        "        self.block1 = nn.ModuleList([shiftedBlock(\n",
        "            dim=embed_dims[1], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias,\n",
        "            qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "            drop_path=dpr[0], norm_layer=norm_layer, sr_ratio=sr_ratios[0])])\n",
        "\n",
        "        self.block2 = nn.ModuleList([shiftedBlock(\n",
        "            dim=embed_dims[2], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias,\n",
        "            qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "            drop_path=dpr[1], norm_layer=norm_layer, sr_ratio=sr_ratios[0])])\n",
        "\n",
        "        self.dblock1 = nn.ModuleList([shiftedBlock(\n",
        "            dim=embed_dims[1], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias,\n",
        "            qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "            drop_path=dpr[0], norm_layer=norm_layer, sr_ratio=sr_ratios[0])])\n",
        "\n",
        "        self.dblock2 = nn.ModuleList([shiftedBlock(\n",
        "            dim=embed_dims[0], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias,\n",
        "            qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "            drop_path=dpr[1], norm_layer=norm_layer, sr_ratio=sr_ratios[0])])\n",
        "\n",
        "        self.patch_embed3 = OverlapPatchEmbed(img_size=img_size // 4, patch_size=3, stride=2,\n",
        "                                              in_chans=embed_dims[0], embed_dim=embed_dims[1])\n",
        "        self.patch_embed4 = OverlapPatchEmbed(img_size=img_size // 8, patch_size=3, stride=2,\n",
        "                                              in_chans=embed_dims[1], embed_dim=embed_dims[2])\n",
        "\n",
        "        self.decoder1 = nn.Conv2d(256, 160, 3, stride=1, padding=1)\n",
        "        self.decoder2 = nn.Conv2d(160, 128, 3, stride=1, padding=1)\n",
        "        self.decoder3 = nn.Conv2d(128, 32, 3, stride=1, padding=1)\n",
        "        self.decoder4 = nn.Conv2d(32, 16, 3, stride=1, padding=1)\n",
        "        self.decoder5 = nn.Conv2d(16, 16, 3, stride=1, padding=1)\n",
        "\n",
        "        self.dbn1 = nn.BatchNorm2d(160)\n",
        "        self.dbn2 = nn.BatchNorm2d(128)\n",
        "        self.dbn3 = nn.BatchNorm2d(32)\n",
        "        self.dbn4 = nn.BatchNorm2d(16)\n",
        "\n",
        "        self.final = nn.Conv2d(16, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "\n",
        "        out = F.relu(F.max_pool2d(self.ebn1(self.encoder1(x)), 2, 2))\n",
        "        t1 = out\n",
        "        out = F.relu(F.max_pool2d(self.ebn2(self.encoder2(out)), 2, 2))\n",
        "        t2 = out\n",
        "        out = F.relu(F.max_pool2d(self.ebn3(self.encoder3(out)), 2, 2))\n",
        "        t3 = out\n",
        "\n",
        "        out, H, W = self.patch_embed3(out)\n",
        "        for blk in self.block1:\n",
        "            out = blk(out, H, W)\n",
        "        out = self.norm3(out)\n",
        "        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
        "        t4 = out\n",
        "\n",
        "        out, H, W = self.patch_embed4(out)\n",
        "        for blk in self.block2:\n",
        "            out = blk(out, H, W)\n",
        "        out = self.norm4(out)\n",
        "        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        out = F.relu(F.interpolate(self.dbn1(self.decoder1(out)), scale_factor=(2, 2), mode='bilinear'))\n",
        "        out = torch.add(out, t4)\n",
        "        _, _, H, W = out.shape\n",
        "        out = out.flatten(2).transpose(1, 2)\n",
        "        for blk in self.dblock1:\n",
        "            out = blk(out, H, W)\n",
        "\n",
        "        out = self.dnorm3(out)\n",
        "        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
        "        out = F.relu(F.interpolate(self.dbn2(self.decoder2(out)), scale_factor=(2, 2), mode='bilinear'))\n",
        "        out = torch.add(out, t3)\n",
        "        _, _, H, W = out.shape\n",
        "        out = out.flatten(2).transpose(1, 2)\n",
        "        for blk in self.dblock2:\n",
        "            out = blk(out, H, W)\n",
        "\n",
        "        out = self.dnorm4(out)\n",
        "        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        out = F.relu(F.interpolate(self.dbn3(self.decoder3(out)), scale_factor=(2, 2), mode='bilinear'))\n",
        "        out = torch.add(out, t2)\n",
        "        out = F.relu(F.interpolate(self.dbn4(self.decoder4(out)), scale_factor=(2, 2), mode='bilinear'))\n",
        "        out = torch.add(out, t1)\n",
        "        out = F.relu(F.interpolate(self.decoder5(out), scale_factor=(2, 2), mode='bilinear'))\n",
        "\n",
        "        out = self.final(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "ZP8L1qJZ62sd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "twwBWOw70y66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import argparse\n",
        "import datetime\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from medpy import metric as medpy_metric\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "#                 Metrics (MedPy)\n",
        "# ============================================================\n",
        "\n",
        "def calculate_metric_percase(pred, gt):\n",
        "    pred = pred.astype(np.uint8)\n",
        "    gt = gt.astype(np.uint8)\n",
        "    pred[pred > 0] = 1\n",
        "    gt[gt > 0] = 1\n",
        "\n",
        "    if gt.sum() == 0:\n",
        "        return np.nan, np.nan, np.nan\n",
        "\n",
        "    if pred.sum() == 0:\n",
        "        return 0.0, 0.0, np.nan\n",
        "\n",
        "    dice = medpy_metric.binary.dc(pred, gt)\n",
        "    jac = medpy_metric.binary.jc(pred, gt)\n",
        "    hd95 = medpy_metric.binary.hd95(pred, gt)\n",
        "    return float(dice), float(jac), float(hd95)\n",
        "\n",
        "\n",
        "def multilabel_metric(pred_2d, gt_2d, num_classes):\n",
        "    metric_list = []\n",
        "    for i in range(1, num_classes):\n",
        "        metric_list.append(calculate_metric_percase(pred_2d == i, gt_2d == i))\n",
        "    return metric_list\n",
        "\n",
        "\n",
        "def init_running_metrics(num_classes: int):\n",
        "    return {\n",
        "        \"dice_sum\": {c: 0.0 for c in range(1, num_classes)},\n",
        "        \"jac_sum\":  {c: 0.0 for c in range(1, num_classes)},\n",
        "        \"hd95_sum\": {c: 0.0 for c in range(1, num_classes)},\n",
        "        \"count\":    {c: 0   for c in range(1, num_classes)},\n",
        "    }\n",
        "\n",
        "\n",
        "def update_running_metrics(running, pred_2d, gt_2d, num_classes: int):\n",
        "    mlist = multilabel_metric(pred_2d, gt_2d, num_classes)\n",
        "    for cls_idx, (dice, jac, hd95) in enumerate(mlist, start=1):\n",
        "        if not np.isnan(dice):\n",
        "            running[\"dice_sum\"][cls_idx] += float(dice)\n",
        "        if not np.isnan(jac):\n",
        "            running[\"jac_sum\"][cls_idx] += float(jac)\n",
        "        if not np.isnan(hd95):\n",
        "            running[\"hd95_sum\"][cls_idx] += float(hd95)\n",
        "        if (not np.isnan(dice)) or (not np.isnan(jac)) or (not np.isnan(hd95)):\n",
        "            running[\"count\"][cls_idx] += 1\n",
        "    return running\n",
        "\n",
        "\n",
        "def finalize_metrics(running, num_classes: int):\n",
        "    per_class = {}\n",
        "    dice_vals, jac_vals, hd95_vals = [], [], []\n",
        "\n",
        "    for c in range(1, num_classes):\n",
        "        cnt = running[\"count\"][c]\n",
        "        if cnt > 0:\n",
        "            d = running[\"dice_sum\"][c] / cnt\n",
        "            j = running[\"jac_sum\"][c] / cnt\n",
        "            h = running[\"hd95_sum\"][c] / cnt\n",
        "        else:\n",
        "            d, j, h = np.nan, np.nan, np.nan\n",
        "\n",
        "        per_class[c] = (float(d) if not np.isnan(d) else np.nan,\n",
        "                        float(j) if not np.isnan(j) else np.nan,\n",
        "                        float(h) if not np.isnan(h) else np.nan)\n",
        "\n",
        "        dice_vals.append(d)\n",
        "        jac_vals.append(j)\n",
        "        hd95_vals.append(h)\n",
        "\n",
        "    macro_dice = float(np.nanmean(dice_vals)) if len(dice_vals) else np.nan\n",
        "    macro_jac  = float(np.nanmean(jac_vals))  if len(jac_vals)  else np.nan\n",
        "    macro_hd95 = float(np.nanmean(hd95_vals)) if len(hd95_vals) else np.nan\n",
        "\n",
        "    return per_class, (macro_dice, macro_jac, macro_hd95)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "#                 Losses (Dice excludes background)\n",
        "# ============================================================\n",
        "\n",
        "class BinaryDiceLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input, targets):\n",
        "        N = targets.size(0)\n",
        "        smooth = 1.0\n",
        "        input_flat = input.view(N, -1)\n",
        "        targets_flat = targets.view(N, -1)\n",
        "        inter = input_flat * targets_flat\n",
        "        dice_eff = (2 * inter.sum(1) + smooth) / (input_flat.sum(1) + targets_flat.sum(1) + smooth)\n",
        "        return 1 - dice_eff.sum() / N\n",
        "\n",
        "\n",
        "class MultiClassDiceLoss(nn.Module):\n",
        "    def __init__(self, class_weights=None, include_background=False):\n",
        "        super().__init__()\n",
        "        self.class_weights = class_weights\n",
        "        self.include_background = include_background\n",
        "        self.bdl = BinaryDiceLoss()\n",
        "\n",
        "    def forward(self, logits, target):\n",
        "        C = logits.shape[1]\n",
        "        target_oh = F.one_hot(target.long(), C).permute(0, 3, 1, 2).contiguous()\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "\n",
        "        start_c = 0 if self.include_background else 1\n",
        "        losses = []\n",
        "        for c in range(start_c, C):\n",
        "            losses.append(self.bdl(probs[:, c], target_oh[:, c]))\n",
        "\n",
        "        losses = torch.stack(losses, dim=0)\n",
        "\n",
        "        if self.class_weights is not None:\n",
        "            w = self.class_weights.to(losses.device)\n",
        "            if not self.include_background:\n",
        "                w = w[1:]\n",
        "            w = w / (w.mean().clamp_min(1e-12))\n",
        "            return (losses * w).mean()\n",
        "\n",
        "        return losses.mean()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "#                 Drive logging + CSV\n",
        "# ============================================================\n",
        "\n",
        "class TeeLogger:\n",
        "    def __init__(self, filename):\n",
        "        self.terminal = sys.stdout\n",
        "        self.log = open(filename, \"a\", encoding=\"utf8\")\n",
        "\n",
        "    def write(self, message):\n",
        "        self.terminal.write(message)\n",
        "        self.log.write(message)\n",
        "\n",
        "    def flush(self):\n",
        "        self.terminal.flush()\n",
        "        self.log.flush()\n",
        "        try:\n",
        "            os.fsync(self.log.fileno())\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "def setup_drive_logging(exp_dir):\n",
        "    logs_dir = os.path.join(exp_dir, \"logs\")\n",
        "    os.makedirs(logs_dir, exist_ok=True)\n",
        "    ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    log_path = os.path.join(logs_dir, f\"train_{ts}.log\")\n",
        "    sys.stdout = TeeLogger(log_path)\n",
        "    return log_path\n",
        "\n",
        "\n",
        "def append_csv(csv_path: str, header: str, line: str):\n",
        "    exists = os.path.isfile(csv_path)\n",
        "    with open(csv_path, \"a\", encoding=\"utf8\") as f:\n",
        "        if not exists:\n",
        "            f.write(header)\n",
        "        f.write(line)\n",
        "        f.flush()\n",
        "        try:\n",
        "            os.fsync(f.fileno())\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "#                 Option C: estimate weights\n",
        "# ============================================================\n",
        "\n",
        "def estimate_ce_weights_from_loader(train_loader, num_classes, device,\n",
        "                                   max_batches=200, clamp_min=0.1, clamp_max=10.0):\n",
        "    counts = torch.zeros(num_classes, dtype=torch.float64)\n",
        "    for k, s in enumerate(train_loader):\n",
        "        if k >= max_batches:\n",
        "            break\n",
        "        y = s[\"label\"].view(-1)\n",
        "        counts += torch.bincount(y, minlength=num_classes).double()\n",
        "    freq = counts / counts.sum().clamp_min(1.0)\n",
        "    w = 1.0 / (freq + 1e-12)\n",
        "    w = w / w.mean().clamp_min(1e-12)\n",
        "    w = w.float()\n",
        "    w = torch.clamp(w, clamp_min, clamp_max).to(device)\n",
        "    return w, counts\n",
        "\n",
        "\n",
        "def estimate_dice_weights_from_ce_weights(ce_w):\n",
        "    w = ce_w.clone().detach().float()\n",
        "    w[0] = min(float(w[0]), 1.0)\n",
        "    w = w / w.mean().clamp_min(1e-12)\n",
        "    return w\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "#                 Test wrapper: volume -> 2D slices\n",
        "# ============================================================\n",
        "\n",
        "class TestVol2DSliceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Wrap a test dataset that returns volumes as:\n",
        "      image: (1, D, H, W) or (D, H, W) or (1, H, W)\n",
        "      label: same shape\n",
        "    and expose it as a 2D-slice dataset:\n",
        "      image: (1, H, W)  (single-channel)\n",
        "      label: (H, W)\n",
        "    \"\"\"\n",
        "    def __init__(self, base_dataset):\n",
        "        self.base = base_dataset\n",
        "        self.index = []  # list of (case_id, slice_id)\n",
        "\n",
        "        for case_id in range(len(self.base)):\n",
        "            s = self.base[case_id]\n",
        "            img = s[\"image\"]\n",
        "            if isinstance(img, np.ndarray):\n",
        "                img = torch.from_numpy(img)\n",
        "            if img.dim() == 4:      # (B,D,H,W) or (C,D,H,W) but in your case (1,D,H,W)\n",
        "                D = img.shape[1]\n",
        "            elif img.dim() == 3:    # (D,H,W) or (C,H,W)\n",
        "                D = img.shape[0]\n",
        "            else:\n",
        "                D = 1\n",
        "\n",
        "            for z in range(D):\n",
        "                self.index.append((case_id, z))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.index)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        case_id, z = self.index[idx]\n",
        "        s = self.base[case_id]\n",
        "\n",
        "        img = s[\"image\"]\n",
        "        lab = s[\"label\"]\n",
        "\n",
        "        if isinstance(img, np.ndarray):\n",
        "            img = torch.from_numpy(img)\n",
        "        if isinstance(lab, np.ndarray):\n",
        "            lab = torch.from_numpy(lab)\n",
        "\n",
        "        if img.dim() == 4:      # (1,D,H,W)\n",
        "            img2d = img[0, z]   # (H,W)\n",
        "        elif img.dim() == 3:    # (D,H,W)\n",
        "            img2d = img[z]\n",
        "        elif img.dim() == 2:\n",
        "            img2d = img\n",
        "        else:\n",
        "            raise RuntimeError(f\"Unsupported image dim: {img.dim()}\")\n",
        "\n",
        "        if lab.dim() == 4:\n",
        "            lab2d = lab[0, z]\n",
        "        elif lab.dim() == 3:\n",
        "            lab2d = lab[z]\n",
        "        elif lab.dim() == 2:\n",
        "            lab2d = lab\n",
        "        else:\n",
        "            raise RuntimeError(f\"Unsupported label dim: {lab.dim()}\")\n",
        "\n",
        "        img2d = img2d.unsqueeze(0).float()  # (1,H,W)\n",
        "        lab2d = lab2d.long()                # (H,W)\n",
        "\n",
        "        return {\"image\": img2d, \"label\": lab2d}\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "#                 2D prep + TEST eval (strict 2D)\n",
        "# ============================================================\n",
        "\n",
        "def _prep_2d_batch(images, labels, img_size, device):\n",
        "    # images: (B,1,H,W) or (B,3,H,W); labels: (B,H,W)\n",
        "    if images.size(1) == 1:\n",
        "        images = images.repeat(1, 3, 1, 1)\n",
        "\n",
        "    if images.shape[-2] != img_size or images.shape[-1] != img_size:\n",
        "        images = F.interpolate(images, size=(img_size, img_size), mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "    if labels.shape[-2] != img_size or labels.shape[-1] != img_size:\n",
        "        labels = labels.unsqueeze(1).float()\n",
        "        labels = F.interpolate(labels, size=(img_size, img_size), mode=\"nearest\")\n",
        "        labels = labels.squeeze(1).long()\n",
        "\n",
        "    imgs = images.to(device, dtype=torch.float32, non_blocking=True)\n",
        "    gts = labels.to(device, dtype=torch.long, non_blocking=True)\n",
        "    return imgs, gts\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_on_test_2d(model, test_loader, num_classes, img_size, device):\n",
        "    model.eval()\n",
        "    running = init_running_metrics(num_classes)\n",
        "\n",
        "    for samples in tqdm(test_loader, total=len(test_loader), desc=\"TEST_2D\"):\n",
        "        images = samples[\"image\"]  # (B,1,H,W)\n",
        "        labels = samples[\"label\"]  # (B,H,W)\n",
        "\n",
        "        imgs, gts = _prep_2d_batch(images, labels, img_size, device)\n",
        "        logits = model(imgs)\n",
        "        pred = torch.argmax(torch.softmax(logits, dim=1), dim=1)\n",
        "\n",
        "        pred_np = pred.detach().cpu().numpy()\n",
        "        gt_np = gts.detach().cpu().numpy()\n",
        "\n",
        "        for b in range(pred_np.shape[0]):\n",
        "            running = update_running_metrics(running, pred_np[b], gt_np[b], num_classes)\n",
        "\n",
        "    per_class, macro = finalize_metrics(running, num_classes)\n",
        "    return per_class, macro\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "#                 Args\n",
        "# ============================================================\n",
        "\n",
        "def get_argparser():\n",
        "    p = argparse.ArgumentParser()\n",
        "\n",
        "    p.add_argument(\"--versions_root\", type=str, default=\"/kaggle/input/synapse\")\n",
        "    p.add_argument(\"--data_subdir\", type=str, default=\"Synapse\")\n",
        "    p.add_argument(\"--list_dir\", type=str, default=\"/root/lists_synapse\")\n",
        "    p.add_argument(\"--train_folder\", type=str, default=\"train_npz\")\n",
        "    p.add_argument(\"--test_folder\", type=str, default=\"test_vol_h5\")\n",
        "\n",
        "    p.add_argument(\"--num_classes\", type=int, default=9)\n",
        "    p.add_argument(\"--START_EPOCH\", type=int, default=0)\n",
        "    p.add_argument(\"--NB_EPOCH\", type=int, default=50)\n",
        "\n",
        "    p.add_argument(\"--LR\", type=float, default=1e-4)\n",
        "    p.add_argument(\"--weight_decay\", type=float, default=1e-4)\n",
        "\n",
        "    p.add_argument(\"--batch_size\", type=int, default=4)\n",
        "    p.add_argument(\"--test_batch_size\", type=int, default=8)\n",
        "    p.add_argument(\"--img_size\", type=int, default=224)\n",
        "\n",
        "    p.add_argument(\"--RESUME\", type=bool, default=False)\n",
        "    p.add_argument(\"--random_seed\", type=int, default=1234)\n",
        "\n",
        "    p.add_argument(\"--metrics_every\", type=int, default=10)\n",
        "    p.add_argument(\"--grad_clip\", type=float, default=1.0)\n",
        "\n",
        "    p.add_argument(\"--drive_root\", type=str, default=\"/content/drive/MyDrive/Synapse_experiments\")\n",
        "    p.add_argument(\"--exp_name\", type=str, default=\"Synapse_UNext_only\")\n",
        "\n",
        "    p.add_argument(\"--ce_weight_batches\", type=int, default=200)\n",
        "    p.add_argument(\"--ce_w_min\", type=float, default=0.1)\n",
        "    p.add_argument(\"--ce_w_max\", type=float, default=10.0)\n",
        "\n",
        "    p.add_argument(\"--save_best_on\", type=str, default=\"macro_dice\", choices=[\"macro_dice\", \"loss\"])\n",
        "\n",
        "    return p\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "#                 Main\n",
        "# ============================================================\n",
        "\n",
        "def main():\n",
        "    opts = get_argparser().parse_known_args()[0]\n",
        "\n",
        "    drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "    exp_dir = os.path.join(opts.drive_root, opts.exp_name)\n",
        "    os.makedirs(exp_dir, exist_ok=True)\n",
        "    ckpt_dir = os.path.join(exp_dir, \"checkpoints\")\n",
        "    os.makedirs(ckpt_dir, exist_ok=True)\n",
        "\n",
        "    log_path = setup_drive_logging(exp_dir)\n",
        "    print(\"✅ Logging to:\", log_path)\n",
        "    print(\"✅ Exp dir:\", exp_dir)\n",
        "\n",
        "    csv_path = os.path.join(exp_dir, \"history_unext.csv\")\n",
        "    C = opts.num_classes\n",
        "\n",
        "    header_cols = [\n",
        "        \"epoch\", \"lr\",\n",
        "        \"train_loss\", \"train_ce\", \"train_dice_loss\",\n",
        "        \"train_macro_dice\", \"train_macro_jac\", \"train_macro_hd95\",\n",
        "        \"test_macro_dice\", \"test_macro_jac\", \"test_macro_hd95\",\n",
        "    ]\n",
        "    for c in range(1, C):\n",
        "        header_cols += [\n",
        "            f\"train_dice_c{c:02d}\", f\"train_jac_c{c:02d}\", f\"train_hd95_c{c:02d}\",\n",
        "            f\"test_dice_c{c:02d}\",  f\"test_jac_c{c:02d}\",  f\"test_hd95_c{c:02d}\",\n",
        "        ]\n",
        "    header = \",\".join(header_cols) + \"\\n\"\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    torch.manual_seed(opts.random_seed)\n",
        "    np.random.seed(opts.random_seed)\n",
        "    random.seed(opts.random_seed)\n",
        "\n",
        "    tr_transform = RandomGenerator(output_size=[opts.img_size, opts.img_size])\n",
        "\n",
        "    train_dataset = Synapse_dataset(\n",
        "        versions_root=opts.versions_root,\n",
        "        split=\"train\",\n",
        "        transform=tr_transform,\n",
        "        list_dir=opts.list_dir,\n",
        "        data_subdir=opts.data_subdir,\n",
        "        train_folder=opts.train_folder,\n",
        "        test_folder=opts.test_folder,\n",
        "        verbose=True,\n",
        "    )\n",
        "    print(\"Train size:\", len(train_dataset))\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=opts.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    # Base test dataset (volumes)\n",
        "    test_base = Synapse_dataset(\n",
        "        versions_root=opts.versions_root,\n",
        "        split=\"test\",\n",
        "        transform=None,\n",
        "        list_dir=opts.list_dir,\n",
        "        data_subdir=opts.data_subdir,\n",
        "        train_folder=opts.train_folder,\n",
        "        test_folder=opts.test_folder,\n",
        "        verbose=True,\n",
        "    )\n",
        "    print(\"Test cases:\", len(test_base))\n",
        "\n",
        "    # Wrap volumes -> 2D slices\n",
        "    test_dataset = TestVol2DSliceDataset(test_base)\n",
        "    print(\"Test slices (2D):\", len(test_dataset))\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=opts.test_batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    model = UNext(num_classes=C, img_size=opts.img_size).to(device)\n",
        "\n",
        "    last_path = os.path.join(ckpt_dir, \"UNext_last.pth\")\n",
        "    best_path = os.path.join(ckpt_dir, \"UNext_best.pth\")\n",
        "\n",
        "    if opts.RESUME and os.path.isfile(last_path):\n",
        "        print(\"Loading UNext checkpoint:\", last_path)\n",
        "        model.load_state_dict(torch.load(last_path, map_location=device), strict=True)\n",
        "\n",
        "    ce_w, counts = estimate_ce_weights_from_loader(\n",
        "        train_loader, C, device,\n",
        "        max_batches=opts.ce_weight_batches,\n",
        "        clamp_min=opts.ce_w_min,\n",
        "        clamp_max=opts.ce_w_max,\n",
        "    )\n",
        "    print(\"Pixel counts:\", counts.cpu().numpy().astype(np.int64))\n",
        "    print(\"CE weights:\", ce_w.detach().cpu().numpy())\n",
        "\n",
        "    ce_loss = nn.CrossEntropyLoss(weight=ce_w, reduction=\"mean\")\n",
        "    dice_w = estimate_dice_weights_from_ce_weights(ce_w)\n",
        "    dice_loss = MultiClassDiceLoss(class_weights=dice_w, include_background=False)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=opts.LR, weight_decay=opts.weight_decay)\n",
        "\n",
        "    best_score = -1e9 if opts.save_best_on == \"macro_dice\" else 1e9\n",
        "\n",
        "    def _fmt(x):\n",
        "        return \"\" if (x is None or (isinstance(x, float) and np.isnan(x))) else f\"{x}\"\n",
        "\n",
        "    for epoch in range(opts.START_EPOCH, opts.NB_EPOCH):\n",
        "        lr = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(f\"Epoch {epoch}/{opts.NB_EPOCH-1} | lr={lr:.6g} | FIXED LR (2D test)\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        model.train()\n",
        "        running_train = init_running_metrics(C)\n",
        "\n",
        "        loss_sum = 0.0\n",
        "        ce_sum = 0.0\n",
        "        dice_sum = 0.0\n",
        "        n_batches = 0\n",
        "\n",
        "        for i, samples in tqdm(enumerate(train_loader), total=len(train_loader), desc=\"TRAIN\"):\n",
        "            images = samples[\"image\"]\n",
        "            labels = samples[\"label\"]\n",
        "\n",
        "            if images.size(1) == 1:\n",
        "                images = images.repeat(1, 3, 1, 1)\n",
        "\n",
        "            imgs = images.to(device, dtype=torch.float32, non_blocking=True)\n",
        "            gts = labels.to(device, dtype=torch.long, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            logits = model(imgs)\n",
        "\n",
        "            loss_ce = ce_loss(logits, gts)\n",
        "            loss_d = dice_loss(logits, gts)\n",
        "            loss = loss_ce + loss_d\n",
        "\n",
        "            loss.backward()\n",
        "            if opts.grad_clip > 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), opts.grad_clip)\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_sum += float(loss.detach().cpu())\n",
        "            ce_sum += float(loss_ce.detach().cpu())\n",
        "            dice_sum += float(loss_d.detach().cpu())\n",
        "            n_batches += 1\n",
        "\n",
        "            if i % 20 == 0:\n",
        "                print(f\"[batch {i:04d}] loss={loss.item():.4f} | ce={loss_ce.item():.4f} | dice={loss_d.item():.4f}\")\n",
        "\n",
        "            if opts.metrics_every > 0 and (i % opts.metrics_every == 0):\n",
        "                with torch.no_grad():\n",
        "                    pred = torch.argmax(torch.softmax(logits, dim=1), dim=1)\n",
        "                    pred_np = pred.detach().cpu().numpy()\n",
        "                    gt_np = gts.detach().cpu().numpy()\n",
        "                    for b in range(pred_np.shape[0]):\n",
        "                        running_train = update_running_metrics(running_train, pred_np[b], gt_np[b], C)\n",
        "\n",
        "        train_loss = loss_sum / max(1, n_batches)\n",
        "        train_ce = ce_sum / max(1, n_batches)\n",
        "        train_dice_l = dice_sum / max(1, n_batches)\n",
        "\n",
        "        train_per_class, train_macro = finalize_metrics(running_train, C)\n",
        "        tr_macro_dice, tr_macro_jac, tr_macro_hd95 = train_macro\n",
        "\n",
        "        print(\"\\n--- Train Summary ---\")\n",
        "        print(f\"Loss={train_loss:.6f} | CE={train_ce:.6f} | DiceLoss={train_dice_l:.6f}\")\n",
        "        print(f\"[TRAIN] Macro Dice={tr_macro_dice:.4f} | Jaccard={tr_macro_jac:.4f} | HD95={tr_macro_hd95:.4f}\")\n",
        "\n",
        "        test_per_class, test_macro = evaluate_on_test_2d(model, test_loader, C, opts.img_size, device)\n",
        "        te_macro_dice, te_macro_jac, te_macro_hd95 = test_macro\n",
        "\n",
        "        print(\"\\n--- Test Summary (2D slices from test_vol) ---\")\n",
        "        print(f\"[TEST] Macro Dice={te_macro_dice:.4f} | Jaccard={te_macro_jac:.4f} | HD95={te_macro_hd95:.4f}\")\n",
        "\n",
        "        torch.save(model.state_dict(), last_path)\n",
        "        print(\"✅ Saved UNext last checkpoint:\", last_path)\n",
        "\n",
        "        row = [\n",
        "            str(epoch), f\"{lr}\",\n",
        "            f\"{train_loss}\", f\"{train_ce}\", f\"{train_dice_l}\",\n",
        "            _fmt(tr_macro_dice), _fmt(tr_macro_jac), _fmt(tr_macro_hd95),\n",
        "            _fmt(te_macro_dice), _fmt(te_macro_jac), _fmt(te_macro_hd95),\n",
        "        ]\n",
        "        for c in range(1, C):\n",
        "            td, tj, th = train_per_class[c]\n",
        "            vd, vj, vh = test_per_class[c]\n",
        "            row += [_fmt(td), _fmt(tj), _fmt(th), _fmt(vd), _fmt(vj), _fmt(vh)]\n",
        "        append_csv(csv_path, header, \",\".join(row) + \"\\n\")\n",
        "\n",
        "        if opts.save_best_on == \"macro_dice\":\n",
        "            score = te_macro_dice\n",
        "            if (not np.isnan(score)) and score > best_score:\n",
        "                best_score = score\n",
        "                torch.save(model.state_dict(), best_path)\n",
        "                print(f\"✅ Saved UNext best checkpoint (test_macro_dice={best_score:.4f}):\", best_path)\n",
        "        else:\n",
        "            score = train_loss\n",
        "            if score < best_score:\n",
        "                best_score = score\n",
        "                torch.save(model.state_dict(), best_path)\n",
        "                print(f\"✅ Saved UNext best checkpoint (loss={best_score:.6f}):\", best_path)\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Done.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vwp4KMcr0z06",
        "outputId": "6f30b25a-d15e-40cf-eb63-23aa820d9246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TRAIN: 100%|██████████| 553/553 [00:55<00:00,  9.99it/s]\n",
            "TEST_2D: 100%|██████████| 196/196 [10:16<00:00,  3.15s/it]\n",
            "TRAIN: 100%|██████████| 553/553 [00:58<00:00,  9.52it/s]\n",
            "TEST_2D: 100%|██████████| 196/196 [09:51<00:00,  3.02s/it]\n",
            "TRAIN: 100%|██████████| 553/553 [00:54<00:00, 10.08it/s]\n",
            "TEST_2D: 100%|██████████| 196/196 [09:39<00:00,  2.96s/it]\n",
            "TRAIN: 100%|██████████| 553/553 [00:54<00:00, 10.21it/s]\n",
            "TEST_2D: 100%|██████████| 196/196 [09:52<00:00,  3.02s/it]\n",
            "TRAIN: 100%|██████████| 553/553 [00:56<00:00,  9.77it/s]\n",
            "TEST_2D: 100%|██████████| 196/196 [10:06<00:00,  3.09s/it]\n",
            "TRAIN: 100%|██████████| 553/553 [00:55<00:00,  9.97it/s]\n",
            "TEST_2D: 100%|██████████| 196/196 [10:34<00:00,  3.24s/it]\n",
            "TRAIN: 100%|██████████| 553/553 [00:58<00:00,  9.38it/s]\n",
            "TEST_2D: 100%|██████████| 196/196 [09:59<00:00,  3.06s/it]\n",
            "TRAIN: 100%|██████████| 553/553 [00:57<00:00,  9.57it/s]\n",
            "TEST_2D: 100%|██████████| 196/196 [09:43<00:00,  2.98s/it]\n",
            "TRAIN: 100%|██████████| 553/553 [00:57<00:00,  9.62it/s]\n",
            "TEST_2D: 100%|██████████| 196/196 [09:44<00:00,  2.98s/it]\n",
            "TRAIN: 100%|██████████| 553/553 [00:56<00:00,  9.76it/s]\n",
            "TEST_2D: 100%|██████████| 196/196 [09:48<00:00,  3.00s/it]\n",
            "TRAIN: 100%|██████████| 553/553 [00:56<00:00,  9.82it/s]\n",
            "TEST_2D: 100%|██████████| 196/196 [10:11<00:00,  3.12s/it]\n",
            "TRAIN: 100%|██████████| 553/553 [00:57<00:00,  9.61it/s]\n",
            "TEST_2D: 100%|██████████| 196/196 [10:09<00:00,  3.11s/it]\n",
            "TRAIN: 100%|██████████| 553/553 [00:57<00:00,  9.63it/s]\n",
            "TEST_2D: 100%|██████████| 196/196 [10:05<00:00,  3.09s/it]\n",
            "TRAIN: 100%|██████████| 553/553 [00:59<00:00,  9.37it/s]\n",
            "TEST_2D: 100%|██████████| 196/196 [09:54<00:00,  3.03s/it]\n",
            "TRAIN: 100%|██████████| 553/553 [00:56<00:00,  9.73it/s]\n",
            "TEST_2D: 100%|██████████| 196/196 [09:57<00:00,  3.05s/it]\n",
            "TRAIN: 100%|██████████| 553/553 [00:54<00:00, 10.07it/s]\n",
            "TEST_2D: 100%|██████████| 196/196 [09:53<00:00,  3.03s/it]\n",
            "TRAIN: 100%|██████████| 553/553 [00:56<00:00,  9.76it/s]\n",
            "TEST_2D: 100%|██████████| 196/196 [09:53<00:00,  3.03s/it]\n",
            "TRAIN: 100%|██████████| 553/553 [00:55<00:00,  9.99it/s]\n",
            "TEST_2D:  60%|██████    | 118/196 [05:49<04:11,  3.22s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HXj_XgwA7A62"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}